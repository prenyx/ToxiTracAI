{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Improvements\n",
    "\n",
    "- Expand map_condition to include additional conditions (e.g., 'intoxicated').\n",
    "- Increase the model complexity and add regularization.\n",
    "- Use cross-validation to assess performance more robustly.\n",
    "- Monitor more metrics and add model checkpointing for optimal weight saving.\n",
    "- Visualize training progress to check for overfitting or underfitting."
   ],
   "id": "3101916a21fd648d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Expand 'map_condition'\n",
    "\n",
    "- aiming to detect intoxicated states\n",
    "- map 'normal' and 'resting' to 0, 'exercising' to 1 and 'intoxicated' to 2\n",
    "- "
   ],
   "id": "492a99a6ec1c5b9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "high_bpm_threshold = 110 if self._condition in ['resting', 'normal'] else 200\n",
    "if self._condition == 'intoxicated':\n",
    "    high_bpm_threshold = 120  # Example threshold for intoxication\n"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def map_condition(condition):\n",
    "    if condition in ['normal', 'resting']:\n",
    "        return 0\n",
    "    elif condition == 'exercising':\n",
    "        return 1\n",
    "    elif condition == 'intoxicated':\n",
    "        return 2\n",
    "    return -1  # Default case for unexpected labels \n"
   ],
   "id": "13a250419a59406c",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Use the ML Model for Intoxication Prediction:\n",
    "\n",
    "If your ML model is trained on labeled data that includes 'intoxicated' versus 'normal' conditions, you can extend predict_condition to detect this specifically.\n",
    "You might need to add 'intoxicated' as a possible label output from the model, then modify predict_condition to handle this outcome:"
   ],
   "id": "39a5abd5ddd25dcb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction = self._model(input_scaled_tensor)\n",
    "condition = 'intoxicated' if tf.argmax(prediction, axis=1).numpy()[0] == 2 else 'normal'\n"
   ],
   "id": "2b0afe316fbc2047",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Increase Model Complexity for Better Classification:\n",
    "\n",
    "Since your model has only two hidden layers, adding more layers or increasing the number of neurons in each layer may improve performance, especially if you have more complex data. Consider adding dropout layers to avoid overfitting."
   ],
   "id": "eac2faf9421e4407"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_dim=X_train.shape[1]),\n",
    "    layers.Dropout(0.3),  # Dropout for regularization\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')  # Adjust output layer to 3 if adding a new class\n",
    "])\n"
   ],
   "id": "7ed85a71d46b70ed",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Optimize Hyperparameters:\n",
    "\n",
    "Experiment with the optimizer, learning rate, batch size, and number of epochs. For example, you could try the Adam optimizer with a lower learning rate or use a learning rate scheduler."
   ],
   "id": "249d8c9420a3dbde"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005), \n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ],
   "id": "1211d31901362158",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Evaluate Model Performance with Cross-Validation:\n",
    "\n",
    "Instead of a single train-test split, use K-fold cross-validation to better understand the model’s performance across different splits."
   ],
   "id": "cc9908dd22eea55e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "for train_idx, test_idx in kfold.split(X):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    # Train model and evaluate accuracy for each fold\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, callbacks=[early_stopping], verbose=0)\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    results.append(score[1])  # Save accuracy score\n",
    "print(\"Cross-validation accuracy scores:\", results)\n",
    "print(\"Mean cross-validation accuracy:\", np.mean(results))\n"
   ],
   "id": "eb4cf1e590d80d85",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Monitor Additional Metrics:\n",
    "\n",
    "Use metrics like Precision, Recall, and F1-score to understand class-specific performance, especially for an imbalanced dataset where one condition (e.g., intoxicated) may be less frequent."
   ],
   "id": "6fbf652b7fe364ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_labels, y_pred_labels, target_names=['Normal', 'Active', 'Intoxicated']))\n"
   ],
   "id": "a7c6000708eb43dd",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Add Model Checkpointing:\n",
    "\n",
    "Save the model at its best-performing epoch with ModelCheckpoint, so if training takes long, you don’t lose the best weights due to a later decrease in performance."
   ],
   "id": "fd543cc463abb6f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, callbacks=[early_stopping, checkpoint])\n"
   ],
   "id": "1df7ffc59f3e9e60",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Visualize Training Performance:\n",
    "\n",
    "Plot the training and validation accuracy and loss over epochs to identify potential overfitting and check if early stopping is effective."
   ],
   "id": "ca06456bee5698be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "e9a2dd5a5873c81f",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Consider Data Augmentation or Synthetic Data:\n",
    "\n",
    "If you have limited samples for some conditions (e.g., intoxicated), consider data augmentation techniques (e.g., adding noise to BPM values) or generating synthetic data to balance the classes."
   ],
   "id": "947ddee2b8dea7fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Modify code\n",
   "id": "73f35bfe114b37d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "normal_data_dir = 'path/to/normal/data'\n",
    "intoxicated_data_dir = 'path/to/intoxicated/data'\n",
    "\n",
    "# Function to load and label data from a directory\n",
    "def load_data_from_directory(directory, label):\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.csv'):  # Adjust if your files have a different extension\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['Category'] = label  # Add a new column for the label\n",
    "            data.append(df)\n",
    "    return pd.concat(data, ignore_index=True)\n",
    "\n",
    "# Load normal and intoxicated data, labeling each\n",
    "normal_data = load_data_from_directory(normal_data_dir, label=0)  # 0 for normal\n",
    "intoxicated_data = load_data_from_directory(intoxicated_data_dir, label=1)  # 1 for intoxicated\n",
    "\n",
    "# Combine both datasets\n",
    "combined_data = pd.concat([normal_data, intoxicated_data], ignore_index=True)\n"
   ],
   "id": "99dedab00bbb8ba1",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Step 2: Prepare Data for Model Training\n",
    "After loading and labeling the data, you can proceed with preprocessing (scaling, splitting, etc.) just like in your initial code."
   ],
   "id": "a29ff4ca24a6da66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(combined_data[['BPM min', 'BPM max']])\n",
    "y = to_categorical(combined_data['Category'])  # Convert labels to categorical\n",
    "\n",
    "# Save the scaler for future use\n",
    "import joblib\n",
    "joblib.dump(scaler, 'scaler_for_model.joblib')\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ],
   "id": "73d3bbde30020192",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Step 3: Model Training (As in Your Original Code)\n",
    "After preparing the data, use your existing model training code with early stopping and other improvements."
   ],
   "id": "b4979a036888f688"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Define and compile the model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_dim=X_train.shape[1]),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')  # 2 output neurons for 'normal' and 'intoxicated'\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Save the model\n",
    "model.save('intoxication_detection_model.keras')\n"
   ],
   "id": "7b5f19b786532c6b",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Step 4: Verify Data Loading and Labeling\n",
    "To confirm that the data has been loaded correctly, check the distribution of labels:\n",
    "This should show counts for each label (e.g., 0 for normal and 1 for intoxicated), confirming that the data from each directory was labeled and combined correctly."
   ],
   "id": "148072ddcd520d90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Label distribution in combined data:\")\n",
    "print(combined_data['Category'].value_counts())\n"
   ],
   "id": "a3718550740fd254",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb  # For handling ECG files\n",
    "from wfdb.processing import gqrs_detect  # R-peak detection\n",
    "from datetime import datetime\n",
    "\n",
    "# Directory containing the intoxicated ECG files\n",
    "intoxicated_data_dir = 'path/to/intoxicated/data'\n",
    "\n",
    "# Prepare a list to hold the extracted data\n",
    "intoxicated_data = []\n",
    "\n",
    "# Process each ECG file in the directory\n",
    "for filename in os.listdir(intoxicated_data_dir):\n",
    "    if filename.endswith('.dat'):\n",
    "        # Construct the record name (without the .dat extension)\n",
    "        record_name = os.path.join(intoxicated_data_dir, filename[:-4])\n",
    "\n",
    "        try:\n",
    "            # Load ECG data\n",
    "            record = wfdb.rdrecord(record_name)\n",
    "            fs = record.fs  # Sampling frequency\n",
    "\n",
    "            # Detect R-peaks\n",
    "            r_peaks = gqrs_detect(sig=record.p_signal[:, 0], fs=fs)  # Assuming the first channel\n",
    "\n",
    "            # Calculate RR intervals and BPM\n",
    "            rr_intervals = np.diff(r_peaks) / fs  # Convert samples to seconds\n",
    "            bpm_values = 60 / rr_intervals\n",
    "\n",
    "            # Calculate BPM statistics\n",
    "            min_bpm = np.min(bpm_values)\n",
    "            max_bpm = np.max(bpm_values)\n",
    "            avg_bpm = np.mean(bpm_values)\n",
    "\n",
    "            # Append data to list\n",
    "            intoxicated_data.append({\n",
    "                \"ID\": filename[:-4],  # Use the filename (without extension) as ID\n",
    "                \"Date\": datetime.now().date(),  # You can use actual date if available\n",
    "                \"BPM min\": min_bpm,\n",
    "                \"BPM max\": max_bpm,\n",
    "                \"BPM avg\": avg_bpm,\n",
    "                \"Condition\": \"intoxicated\"\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Convert the data list to a DataFrame\n",
    "intoxicated_df = pd.DataFrame(intoxicated_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = 'intoxicated_bpm_data.csv'\n",
    "intoxicated_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Intoxicated BPM data saved to {output_csv_path}\")\n"
   ],
   "id": "24f218754622b6ae",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
